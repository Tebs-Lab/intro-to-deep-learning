{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Frozen Lake With Q-Learning\n",
    "\n",
    "In this notebook we'll implement q-learning on a simple discrete environment called Frozen Lake. Here is what the documentation says about Frozen Lake:\n",
    "\n",
    "> Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "> The surface is described using a grid like the following:\n",
    "\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```\n",
    "\n",
    "> The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "The documentation on OpenAI is infuriatingly sparse, but the environments and API is incredibly helpful. Get used to digging around in the [source code](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py) if you want to fully understand the environments. For example, the source indicates which actions are which:\n",
    "\n",
    "```\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "```\n",
    "\n",
    "And this is what they mean when they say the \"ice is slippery\":\n",
    "\n",
    "```python\n",
    "# ...\n",
    "# Code above is nested for loops to address each state/action pair \n",
    "if is_slippery:\n",
    "    for b in [(a-1)%4, a, (a+1)%4]:\n",
    "        newrow, newcol = inc(row, col, b)\n",
    "        newstate = to_s(newrow, newcol)\n",
    "        newletter = desc[newrow, newcol]\n",
    "        done = bytes(newletter) in b'GH'\n",
    "        rew = float(newletter == b'G')\n",
    "        li.append((1.0/3.0, newstate, rew, done))\n",
    "else:\n",
    "    newrow, newcol = inc(row, col, a)\n",
    "    newstate = to_s(newrow, newcol)\n",
    "    newletter = desc[newrow, newcol]\n",
    "    done = bytes(newletter) in b'GH'\n",
    "    rew = float(newletter == b'G')\n",
    "    li.append((1.0, newstate, rew, done))\n",
    "```\n",
    "\n",
    "This line is the most important one: `[(a-1)%4, a, (a+1)%4]`. It indicates that taking an action might result in one of three actions being taken, and what those actions could be. Consider `left=0`: you have a 1 in 3 chance each of actually going left, up, or down. Because:\n",
    "\n",
    "```\n",
    "0-1 % 4 = 3 => UP\n",
    "0+1 % 4 = 1 => DOWN\n",
    "```\n",
    "\n",
    "When the ice is slipperty every action might send you in one of the two perpindicular directions. Specifically:\n",
    "\n",
    "```\n",
    "LEFT  ->  LEFT, UP, DOWN\n",
    "RIGHT ->  RIGHT, UP, DOWN\n",
    "UP    ->  UP, LEFT, RIGHT\n",
    "DOWN  ->  DOWN, LEFT, RIGHT\n",
    "```\n",
    "\n",
    "This is critical knowledge because it dramatically impacts the optimal policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Q-learning is an algorithm that iteratively explores an environment in order to experimentally approximate the Bellman equation: \n",
    "\n",
    "![](images/q-bellman.png)\n",
    "\n",
    "With Q-Learning, we have our agent switch between \"exploring\" the state space and \"exploiting\" the optimal policy (at least, our current notion of that policy). The below is an example of an \"epsilon greedy\" Q-learning algorithm, which means some percent of the time our agent picks an action at random, and the rest of the time it picks whatever our current \"optimal\" action is, defined by the Q-Table. We define an \"exploration rate\" to control how often the agent explores at random, this exploration rate is often denoted by the symbol epsilon (ε), hence \"epsilon greedy\".\n",
    "\n",
    "Every time our agent takes an action, we will update our Q-Table, which contains an entry for every state-action pair that our agent encounters. We initialize this table to be 0 for all state-action pairs. Additionally, Q-Learning introduces a \"learning rate\" which is similar to the learning rate in Stochastic Gradient Descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 avarage reward: 0.0\n",
      "Episode 500 avarage reward: 0.0\n",
      "Episode 1000 avarage reward: 0.0\n",
      "Episode 1500 avarage reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Before we do the harder, stocastic, version of this problem\n",
    "# Lets appreciate how fast Q-Learning converges on an optimal \n",
    "# policy when the action's it takes are always respected.\n",
    "\n",
    "# Initialize the environment, set is_slippery to false.\n",
    "environment = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "\n",
    "# The Q-Table is a map of state-action pairs to the estimated value of \n",
    "# that state-action pair. In this game there are 16 states and 4 actions\n",
    "# per state. This simple 2D array captures all the state-action pairs \n",
    "# nicely. And we don't know anything about the values yet so we use 0\n",
    "# as the initial value for all state-action pairs. \n",
    "q_table = np.zeros([environment.observation_space.n, environment.action_space.n])\n",
    "\n",
    "# Some global parameters for Q-Learning.\n",
    "# These are like network hyperparameters, they might change per task\n",
    "# and we might even change them during the training episodes.\n",
    "learning_rate = 0.01 \n",
    "discount_factor = 0.95\n",
    "exploration_rate = 0.3\n",
    "\n",
    "# Values to constrain training, sometimes we might train until we \n",
    "# detect policy convergance, sometimes we'll train a fixed number\n",
    "# of episodes. Right now we're doing the latter. \n",
    "training_episodes = 10000\n",
    "\n",
    "# To help us evaluate the performance\n",
    "episodes_per_evaluation = 500\n",
    "\n",
    "for current_episode_num in range(training_episodes):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # First decide if we're taking a random action or the current\n",
    "        # most valuable action.\n",
    "        explore = np.random.random() < exploration_rate\n",
    "\n",
    "        if explore:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            # If we're not exploring randomly, we need to examine the Q-table \n",
    "            # to determine the best possible action given the current state\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        # Regardless of how we got the action, take it!\n",
    "        next_state, reward, done, _ = environment.step(action)\n",
    "\n",
    "        # Based on the reward we got for taking that action\n",
    "        # update the Q-table entry for the pair (state, action)\n",
    "        # NOTE: we are NOT updating the value for next_state\n",
    "        # Further note, I've split what is often represented in one \n",
    "        # formula across a couple lines for readability\n",
    "        prev_q_value = q_table[state, action]\n",
    "        discounted_future_reward = discount_factor * np.max(q_table[next_state])\n",
    "\n",
    "        q_table[state, action] = (\n",
    "            prev_q_value + (learning_rate * (reward + discounted_future_reward - prev_q_value))\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Update the state for the next round.\n",
    "        # CRITICAL, don't forget this!\n",
    "        state = next_state\n",
    "    \n",
    "    # Every so often try 100 games and see the average reward. \n",
    "    if current_episode_num % episodes_per_evaluation == 0:\n",
    "        rew_average = 0.\n",
    "        for i in range(100):\n",
    "            obs = environment.reset()\n",
    "            done = False\n",
    "            while done != True: \n",
    "                action = np.argmax(q_table[obs])\n",
    "                obs, rew, done, info = environment.step(action) #take step using selected action\n",
    "                rew_average += rew\n",
    "        rew_average=rew_average/100\n",
    "        print(f'Episode {current_episode_num} avarage reward: {rew_average}')\n",
    "        \n",
    "        # 0.8 is considered solved since 20% of the time the agents\n",
    "        # move is ignored which can cause us to fall into a lake\n",
    "        # So we'll break here with an \"optimal\" policy.\n",
    "        if rew_average > 0.8:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.36742066e-03 8.58158699e-02 7.90981508e-04 8.80274393e-03]\n",
      " [8.34693625e-03 0.00000000e+00 6.04772155e-07 8.79286961e-05]\n",
      " [5.38116595e-05 1.07573947e-03 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.01454898e-02 1.60588692e-01 0.00000000e+00 8.61722404e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 6.25651648e-02 0.00000000e+00 5.13543969e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.40081506e-02 0.00000000e+00 2.81357613e-01 1.24308323e-02]\n",
      " [1.69223492e-02 1.73690977e-02 4.64264220e-01 0.00000000e+00]\n",
      " [4.27959333e-02 7.00260734e-01 0.00000000e+00 3.73628074e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.71102569e-03 2.05746983e-01 3.43170713e-03]\n",
      " [1.28387943e-02 1.70366701e-01 9.25951647e-01 7.99297740e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "∨<∨<\n",
      "∨<∨<\n",
      ">>∨<\n",
      "<>><\n",
      "\n",
      "SFFF \n",
      "FHFH \n",
      "FFFH \n",
      "HFFG \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We quickly learn to play the game perfectly.\n",
    "# Which is not surprising, we could have just\n",
    "# used graph search to be honest. \n",
    "\n",
    "# Still, it's informative to look at the Q-table and visualize the policy\n",
    "print(q_table)\n",
    "\n",
    "# This is the board:\n",
    "\n",
    "# SFFF       (S: starting point, safe)\n",
    "# FHFH       (F: frozen surface, safe)\n",
    "# FFFH       (H: hole, fall to your doom)\n",
    "# HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "actions=['<', '∨', '>', '∧']\n",
    "policy_string = ''\n",
    "\n",
    "for location_id, q_values in enumerate(q_table):\n",
    "    if location_id % 4 == 0: \n",
    "        policy_string += '\\n'\n",
    "    idx = np.argmax(q_values)\n",
    "    policy_action = actions[idx]\n",
    "    policy_string += policy_action\n",
    "\n",
    "print(policy_string)\n",
    "print('''\n",
    "SFFF \n",
    "FHFH \n",
    "FFFH \n",
    "HFFG \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 avarage reward: 0.0\n",
      "Episode 5000 avarage reward: 0.07\n",
      "Episode 10000 avarage reward: 0.39\n",
      "Episode 15000 avarage reward: 0.76\n",
      "Episode 20000 avarage reward: 0.78\n",
      "Episode 25000 avarage reward: 0.72\n",
      "Episode 30000 avarage reward: 0.77\n",
      "Episode 35000 avarage reward: 0.76\n",
      "Episode 40000 avarage reward: 0.77\n",
      "Episode 45000 avarage reward: 0.75\n",
      "Episode 50000 avarage reward: 0.72\n",
      "Episode 55000 avarage reward: 0.74\n",
      "Episode 60000 avarage reward: 0.74\n",
      "Episode 65000 avarage reward: 0.74\n",
      "Episode 70000 avarage reward: 0.78\n",
      "Episode 75000 avarage reward: 0.65\n",
      "Episode 80000 avarage reward: 0.73\n",
      "Episode 85000 avarage reward: 0.72\n",
      "Episode 90000 avarage reward: 0.7\n",
      "Episode 95000 avarage reward: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment, note that we are not using the monitor here\n",
    "# because we can train much faster if we don't render anything. \n",
    "environment = gym.make('FrozenLake-v0', is_slippery=True)\n",
    "\n",
    "# The Q-Table is a map of state-action pairs to the estimated value of \n",
    "# that state-action pair. Being in a state very close to the winning state\n",
    "# and taking an action that leads to victory should be highly valued\n",
    "# whereas being in a state where you're about to lose and taking an \n",
    "# action that leads to a loss should have a low value. But before \n",
    "# we know anything, we just set all the values to 0. \n",
    "q_table = np.zeros([environment.observation_space.n, environment.action_space.n])\n",
    "\n",
    "# Some global parameters for Q-Learning\n",
    "learning_rate = 0.01 \n",
    "discount_factor = 0.95\n",
    "exploration_rate = 0.3\n",
    "\n",
    "# Values to constrain training\n",
    "# Note the very large number...\n",
    "training_episodes = 200000\n",
    "\n",
    "# To help us evaluate the performance\n",
    "episodes_per_evaluation = 5000\n",
    "\n",
    "for current_episode_num in range(training_episodes):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # First decide if we're taking a random action or the current\n",
    "        # most valuable action.\n",
    "        explore = np.random.random() < exploration_rate\n",
    "\n",
    "        if explore:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            # If we're not exploring randomly, we need to examine the Q-table \n",
    "            # to determine the best possible action given the current state\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        # Regardless of how we got the action, take it!\n",
    "        next_state, reward, done, _ = environment.step(action)\n",
    "\n",
    "        # Based on the reward we got for taking that action\n",
    "        # update the Q-table entry for the pair (state, action)\n",
    "        # NOTE: we are NOT updating the value for next_state\n",
    "        # Further note, I've split what is often represented in one \n",
    "        # formula across a couple lines for readability\n",
    "        prev_q_value = q_table[state, action]\n",
    "        discounted_future_reward = discount_factor * np.max(q_table[next_state])\n",
    "\n",
    "        q_table[state, action] = (\n",
    "            prev_q_value + (learning_rate * (reward + discounted_future_reward - prev_q_value))\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Update the state for the next round.\n",
    "        # CRITICAL, don't forget this!\n",
    "        state = next_state\n",
    "    \n",
    "    # Every so often try 100 games and see the average reward. \n",
    "    if current_episode_num % episodes_per_evaluation == 0:\n",
    "        #report every 5000 steps, test 100 games to get avarage point score for statistics and verify if it is solved\n",
    "        rew_average = 0.\n",
    "        for i in range(100):\n",
    "            obs = environment.reset()\n",
    "            done = False\n",
    "            while done != True: \n",
    "                action = np.argmax(q_table[obs])\n",
    "                obs, rew, done, info = environment.step(action) #take step using selected action\n",
    "                rew_average += rew\n",
    "        rew_average=rew_average/100\n",
    "        print(f'Episode {current_episode_num} avarage reward: {rew_average}')\n",
    "        \n",
    "        # 0.8 is considered solved since somtimes the agents\n",
    "        # move is ignored which can cause us to fall into a lake\n",
    "        # So we'll break here with an \"optimal\" policy.\n",
    "        if rew_average > 0.8:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17863465 0.17529071 0.17665382 0.16806531]\n",
      " [0.11126537 0.10709323 0.10223171 0.16080495]\n",
      " [0.15573996 0.14947138 0.14955051 0.14325015]\n",
      " [0.09007912 0.09374907 0.08560954 0.13760651]\n",
      " [0.20404427 0.16032428 0.15207944 0.11783304]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.17366802 0.1141102  0.16626483 0.0509508 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.14665247 0.20041093 0.16856191 0.26760817]\n",
      " [0.26725309 0.36542711 0.30874359 0.20794106]\n",
      " [0.3914147  0.33670748 0.28983959 0.18708371]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.2908254  0.41138156 0.51232013 0.36006388]\n",
      " [0.5265021  0.7570494  0.68815999 0.63438137]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "<∧<∧\n",
      "<<<<\n",
      "∧∨<<\n",
      "<>∨<\n",
      "\n",
      "SFFF \n",
      "FHFH \n",
      "FFFH \n",
      "HFFG \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hmmmm now we learn quite slowly, and never get a\n",
    "# perfect score.\n",
    "print(q_table)\n",
    "\n",
    "# This is the board:\n",
    "\n",
    "# SFFF       (S: starting point, safe)\n",
    "# FHFH       (F: frozen surface, safe)\n",
    "# FFFH       (H: hole, fall to your doom)\n",
    "# HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "actions=['<', '∨', '>', '∧']\n",
    "policy_string = ''\n",
    "\n",
    "for location_id, q_values in enumerate(q_table):\n",
    "    if location_id % 4 == 0: \n",
    "        policy_string += '\\n'\n",
    "    idx = np.argmax(q_values)\n",
    "    policy_action = actions[idx]\n",
    "    policy_string += policy_action\n",
    "\n",
    "print(policy_string)\n",
    "print('''\n",
    "SFFF \n",
    "FHFH \n",
    "FFFH \n",
    "HFFG \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Does that policy table make sense to you?\n",
    "# Why doesn't this match the policy we saw before?\n",
    "\n",
    "# I would agrue the AI is actually behaving very rationally here...\n",
    "\n",
    "# With neural networks, after training we had a trained model\n",
    "# In Q-Learning after training we have a policy that can be followed\n",
    "# by an agent. We can use the agent by removing the exploration factor\n",
    "# and not updating the values in the q-table.\n",
    "\n",
    "# Lets visualize a single playthrough.\n",
    "state = environment.reset()\n",
    "for _ in range(max_steps_per_episode):\n",
    "\n",
    "    action = np.argmax(q_table[state, :])\n",
    "    state, reward, done, _ = environment.step(action)\n",
    "    environment.render()\n",
    "    \n",
    "    # If the game finished before our max number of rounds, break out\n",
    "    if done: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The AI always chooses an action that ensures it won't fall in a hole!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
